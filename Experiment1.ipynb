{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import codecs\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from nmt import attention_model\n",
    "from nmt import gnmt_model\n",
    "from nmt import model as nmt_model\n",
    "from nmt import model_helper\n",
    "from nmt.utils import misc_utils as utils\n",
    "from nmt.utils import nmt_utils\n",
    "from nmt.utils import vocab_utils\n",
    "from nmt.inference import inference\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_hparams(\n",
    "    out_dir, default_hparams, hparams_path, save_hparams=True):\n",
    "    \"\"\"Create hparams or load hparams from out_dir.\"\"\"\n",
    "    hparams = utils.load_hparams(out_dir)\n",
    "    if not hparams:\n",
    "        hparams = default_hparams\n",
    "        hparams = utils.maybe_parse_standard_hparams(\n",
    "            hparams, hparams_path)\n",
    "    else:\n",
    "        hparams = ensure_compatible_hparams(hparams, default_hparams, hparams_path)\n",
    "    hparams = extend_hparams(hparams)\n",
    "\n",
    "    # Save HParams\n",
    "    if save_hparams:\n",
    "        utils.save_hparams(out_dir, hparams)\n",
    "        for metric in hparams.metrics:\n",
    "            utils.save_hparams(getattr(hparams, \"best_\" + metric + \"_dir\"), hparams)\n",
    "\n",
    "    # Print HParams\n",
    "    utils.print_hparams(hparams)\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/emrys/Data/nmt/deen_gnmt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = '/home/emrys/Data/nmt/deen_gnmt/translate.ckpt-163000'\n",
    "ckpt_dir = os.path.dirname(ckpt)\n",
    "ckpt_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hparams(flags):\n",
    "  \"\"\"Create training hparams.\"\"\"\n",
    "  return tf.contrib.training.HParams(\n",
    "      # Data\n",
    "      src=flags.src,\n",
    "      tgt=flags.tgt,\n",
    "      train_prefix=flags.train_prefix,\n",
    "      dev_prefix=flags.dev_prefix,\n",
    "      test_prefix=flags.test_prefix,\n",
    "      vocab_prefix=flags.vocab_prefix,\n",
    "      embed_prefix=flags.embed_prefix,\n",
    "      out_dir=flags.out_dir,\n",
    "\n",
    "      # Networks\n",
    "      num_units=flags.num_units,\n",
    "      num_encoder_layers=(flags.num_encoder_layers or flags.num_layers),\n",
    "      num_decoder_layers=(flags.num_decoder_layers or flags.num_layers),\n",
    "      dropout=flags.dropout,\n",
    "      unit_type=flags.unit_type,\n",
    "      encoder_type=flags.encoder_type,\n",
    "      residual=flags.residual,\n",
    "      time_major=flags.time_major,\n",
    "      num_embeddings_partitions=flags.num_embeddings_partitions,\n",
    "\n",
    "      # Attention mechanisms\n",
    "      attention=flags.attention,\n",
    "      attention_architecture=flags.attention_architecture,\n",
    "      output_attention=flags.output_attention,\n",
    "      pass_hidden_state=flags.pass_hidden_state,\n",
    "\n",
    "      # Train\n",
    "      optimizer=flags.optimizer,\n",
    "      num_train_steps=flags.num_train_steps,\n",
    "      batch_size=flags.batch_size,\n",
    "      init_op=flags.init_op,\n",
    "      init_weight=flags.init_weight,\n",
    "      max_gradient_norm=flags.max_gradient_norm,\n",
    "      learning_rate=flags.learning_rate,\n",
    "      warmup_steps=flags.warmup_steps,\n",
    "      warmup_scheme=flags.warmup_scheme,\n",
    "      decay_scheme=flags.decay_scheme,\n",
    "      colocate_gradients_with_ops=flags.colocate_gradients_with_ops,\n",
    "      num_sampled_softmax=flags.num_sampled_softmax,\n",
    "\n",
    "      # Data constraints\n",
    "      num_buckets=flags.num_buckets,\n",
    "      max_train=flags.max_train,\n",
    "      src_max_len=flags.src_max_len,\n",
    "      tgt_max_len=flags.tgt_max_len,\n",
    "\n",
    "      # Inference\n",
    "      src_max_len_infer=flags.src_max_len_infer,\n",
    "      tgt_max_len_infer=flags.tgt_max_len_infer,\n",
    "      infer_batch_size=flags.infer_batch_size,\n",
    "\n",
    "      # Advanced inference arguments\n",
    "      infer_mode=flags.infer_mode,\n",
    "      beam_width=flags.beam_width,\n",
    "      length_penalty_weight=flags.length_penalty_weight,\n",
    "      sampling_temperature=flags.sampling_temperature,\n",
    "      num_translations_per_input=flags.num_translations_per_input,\n",
    "\n",
    "      # Vocab\n",
    "      sos=flags.sos if flags.sos else vocab_utils.SOS,\n",
    "      eos=flags.eos if flags.eos else vocab_utils.EOS,\n",
    "      subword_option=flags.subword_option,\n",
    "      check_special_token=flags.check_special_token,\n",
    "      use_char_encode=flags.use_char_encode,\n",
    "\n",
    "      # Misc\n",
    "      forget_bias=flags.forget_bias,\n",
    "      num_gpus=flags.num_gpus,\n",
    "      epoch_step=0,  # record where we were within an epoch.\n",
    "      steps_per_stats=flags.steps_per_stats,\n",
    "      steps_per_external_eval=flags.steps_per_external_eval,\n",
    "      share_vocab=flags.share_vocab,\n",
    "      metrics=flags.metrics.split(\",\"),\n",
    "      log_device_placement=flags.log_device_placement,\n",
    "      random_seed=flags.random_seed,\n",
    "      override_loaded_hparams=flags.override_loaded_hparams,\n",
    "      num_keep_ckpts=flags.num_keep_ckpts,\n",
    "      avg_ckpts=flags.avg_ckpts,\n",
    "      language_model=flags.language_model,\n",
    "      num_intra_threads=flags.num_intra_threads,\n",
    "      num_inter_threads=flags.num_inter_threads,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arguments(parser):\n",
    "  \"\"\"Build ArgumentParser.\"\"\"\n",
    "  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "\n",
    "  # network\n",
    "  parser.add_argument(\"--num_units\", type=int, default=32, help=\"Network size.\")\n",
    "  parser.add_argument(\"--num_layers\", type=int, default=2,\n",
    "                      help=\"Network depth.\")\n",
    "  parser.add_argument(\"--num_encoder_layers\", type=int, default=None,\n",
    "                      help=\"Encoder depth, equal to num_layers if None.\")\n",
    "  parser.add_argument(\"--num_decoder_layers\", type=int, default=None,\n",
    "                      help=\"Decoder depth, equal to num_layers if None.\")\n",
    "  parser.add_argument(\"--encoder_type\", type=str, default=\"uni\", help=\"\"\"\\\n",
    "      uni | bi | gnmt.\n",
    "      For bi, we build num_encoder_layers/2 bi-directional layers.\n",
    "      For gnmt, we build 1 bi-directional layer, and (num_encoder_layers - 1)\n",
    "        uni-directional layers.\\\n",
    "      \"\"\")\n",
    "  parser.add_argument(\"--residual\", type=\"bool\", nargs=\"?\", const=True,\n",
    "                      default=False,\n",
    "                      help=\"Whether to add residual connections.\")\n",
    "  parser.add_argument(\"--time_major\", type=\"bool\", nargs=\"?\", const=True,\n",
    "                      default=True,\n",
    "                      help=\"Whether to use time-major mode for dynamic RNN.\")\n",
    "  parser.add_argument(\"--num_embeddings_partitions\", type=int, default=0,\n",
    "                      help=\"Number of partitions for embedding vars.\")\n",
    "\n",
    "  # attention mechanisms\n",
    "  parser.add_argument(\"--attention\", type=str, default=\"\", help=\"\"\"\\\n",
    "      luong | scaled_luong | bahdanau | normed_bahdanau or set to \"\" for no\n",
    "      attention\\\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      \"--attention_architecture\",\n",
    "      type=str,\n",
    "      default=\"standard\",\n",
    "      help=\"\"\"\\\n",
    "      standard | gnmt | gnmt_v2.\n",
    "      standard: use top layer to compute attention.\n",
    "      gnmt: GNMT style of computing attention, use previous bottom layer to\n",
    "          compute attention.\n",
    "      gnmt_v2: similar to gnmt, but use current bottom layer to compute\n",
    "          attention.\\\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      \"--output_attention\", type=\"bool\", nargs=\"?\", const=True,\n",
    "      default=True,\n",
    "      help=\"\"\"\\\n",
    "      Only used in standard attention_architecture. Whether use attention as\n",
    "      the cell output at each timestep.\n",
    "      .\\\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      \"--pass_hidden_state\", type=\"bool\", nargs=\"?\", const=True,\n",
    "      default=True,\n",
    "      help=\"\"\"\\\n",
    "      Whether to pass encoder's hidden state to decoder when using an attention\n",
    "      based model.\\\n",
    "      \"\"\")\n",
    "\n",
    "  # optimizer\n",
    "  parser.add_argument(\"--optimizer\", type=str, default=\"sgd\", help=\"sgd | adam\")\n",
    "  parser.add_argument(\"--learning_rate\", type=float, default=1.0,\n",
    "                      help=\"Learning rate. Adam: 0.001 | 0.0001\")\n",
    "  parser.add_argument(\"--warmup_steps\", type=int, default=0,\n",
    "                      help=\"How many steps we inverse-decay learning.\")\n",
    "  parser.add_argument(\"--warmup_scheme\", type=str, default=\"t2t\", help=\"\"\"\\\n",
    "      How to warmup learning rates. Options include:\n",
    "        t2t: Tensor2Tensor's way, start with lr 100 times smaller, then\n",
    "             exponentiate until the specified lr.\\\n",
    "      \"\"\")\n",
    "  parser.add_argument(\n",
    "      \"--decay_scheme\", type=str, default=\"\", help=\"\"\"\\\n",
    "      How we decay learning rate. Options include:\n",
    "        luong234: after 2/3 num train steps, we start halving the learning rate\n",
    "          for 4 times before finishing.\n",
    "        luong5: after 1/2 num train steps, we start halving the learning rate\n",
    "          for 5 times before finishing.\\\n",
    "        luong10: after 1/2 num train steps, we start halving the learning rate\n",
    "          for 10 times before finishing.\\\n",
    "      \"\"\")\n",
    "\n",
    "  parser.add_argument(\n",
    "      \"--num_train_steps\", type=int, default=12000, help=\"Num steps to train.\")\n",
    "  parser.add_argument(\"--colocate_gradients_with_ops\", type=\"bool\", nargs=\"?\",\n",
    "                      const=True,\n",
    "                      default=True,\n",
    "                      help=(\"Whether try colocating gradients with \"\n",
    "                            \"corresponding op\"))\n",
    "\n",
    "  # initializer\n",
    "  parser.add_argument(\"--init_op\", type=str, default=\"uniform\",\n",
    "                      help=\"uniform | glorot_normal | glorot_uniform\")\n",
    "  parser.add_argument(\"--init_weight\", type=float, default=0.1,\n",
    "                      help=(\"for uniform init_op, initialize weights \"\n",
    "                            \"between [-this, this].\"))\n",
    "\n",
    "  # data\n",
    "  parser.add_argument(\"--src\", type=str, default=None,\n",
    "                      help=\"Source suffix, e.g., en.\")\n",
    "  parser.add_argument(\"--tgt\", type=str, default=None,\n",
    "                      help=\"Target suffix, e.g., de.\")\n",
    "  parser.add_argument(\"--train_prefix\", type=str, default=None,\n",
    "                      help=\"Train prefix, expect files with src/tgt suffixes.\")\n",
    "  parser.add_argument(\"--dev_prefix\", type=str, default=None,\n",
    "                      help=\"Dev prefix, expect files with src/tgt suffixes.\")\n",
    "  parser.add_argument(\"--test_prefix\", type=str, default=None,\n",
    "                      help=\"Test prefix, expect files with src/tgt suffixes.\")\n",
    "  parser.add_argument(\"--out_dir\", type=str, default=None,\n",
    "                      help=\"Store log/model files.\")\n",
    "\n",
    "  # Vocab\n",
    "  parser.add_argument(\"--vocab_prefix\", type=str, default=None, help=\"\"\"\\\n",
    "      Vocab prefix, expect files with src/tgt suffixes.\\\n",
    "      \"\"\")\n",
    "  parser.add_argument(\"--embed_prefix\", type=str, default=None, help=\"\"\"\\\n",
    "      Pretrained embedding prefix, expect files with src/tgt suffixes.\n",
    "      The embedding files should be Glove formated txt files.\\\n",
    "      \"\"\")\n",
    "  parser.add_argument(\"--sos\", type=str, default=\"<s>\",\n",
    "                      help=\"Start-of-sentence symbol.\")\n",
    "  parser.add_argument(\"--eos\", type=str, default=\"</s>\",\n",
    "                      help=\"End-of-sentence symbol.\")\n",
    "  parser.add_argument(\"--share_vocab\", type=\"bool\", nargs=\"?\", const=True,\n",
    "                      default=False,\n",
    "                      help=\"\"\"\\\n",
    "      Whether to use the source vocab and embeddings for both source and\n",
    "      target.\\\n",
    "      \"\"\")\n",
    "  parser.add_argument(\"--check_special_token\", type=\"bool\", default=True,\n",
    "                      help=\"\"\"\\\n",
    "                      Whether check special sos, eos, unk tokens exist in the\n",
    "                      vocab files.\\\n",
    "                      \"\"\")\n",
    "\n",
    "  # Sequence lengths\n",
    "  parser.add_argument(\"--src_max_len\", type=int, default=50,\n",
    "                      help=\"Max length of src sequences during training.\")\n",
    "  parser.add_argument(\"--tgt_max_len\", type=int, default=50,\n",
    "                      help=\"Max length of tgt sequences during training.\")\n",
    "  parser.add_argument(\"--src_max_len_infer\", type=int, default=None,\n",
    "                      help=\"Max length of src sequences during inference.\")\n",
    "  parser.add_argument(\"--tgt_max_len_infer\", type=int, default=None,\n",
    "                      help=\"\"\"\\\n",
    "      Max length of tgt sequences during inference.  Also use to restrict the\n",
    "      maximum decoding length.\\\n",
    "      \"\"\")\n",
    "\n",
    "  # Default settings works well (rarely need to change)\n",
    "  parser.add_argument(\"--unit_type\", type=str, default=\"lstm\",\n",
    "                      help=\"lstm | gru | layer_norm_lstm | nas\")\n",
    "  parser.add_argument(\"--forget_bias\", type=float, default=1.0,\n",
    "                      help=\"Forget bias for BasicLSTMCell.\")\n",
    "  parser.add_argument(\"--dropout\", type=float, default=0.2,\n",
    "                      help=\"Dropout rate (not keep_prob)\")\n",
    "  parser.add_argument(\"--max_gradient_norm\", type=float, default=5.0,\n",
    "                      help=\"Clip gradients to this norm.\")\n",
    "  parser.add_argument(\"--batch_size\", type=int, default=128, help=\"Batch size.\")\n",
    "\n",
    "  parser.add_argument(\"--steps_per_stats\", type=int, default=100,\n",
    "                      help=(\"How many training steps to do per stats logging.\"\n",
    "                            \"Save checkpoint every 10x steps_per_stats\"))\n",
    "  parser.add_argument(\"--max_train\", type=int, default=0,\n",
    "                      help=\"Limit on the size of training data (0: no limit).\")\n",
    "  parser.add_argument(\"--num_buckets\", type=int, default=5,\n",
    "                      help=\"Put data into similar-length buckets.\")\n",
    "  parser.add_argument(\"--num_sampled_softmax\", type=int, default=0,\n",
    "                      help=(\"Use sampled_softmax_loss if > 0.\"\n",
    "                            \"Otherwise, use full softmax loss.\"))\n",
    "\n",
    "  # SPM\n",
    "  parser.add_argument(\"--subword_option\", type=str, default=\"\",\n",
    "                      choices=[\"\", \"bpe\", \"spm\"],\n",
    "                      help=\"\"\"\\\n",
    "                      Set to bpe or spm to activate subword desegmentation.\\\n",
    "                      \"\"\")\n",
    "\n",
    "  # Experimental encoding feature.\n",
    "  parser.add_argument(\"--use_char_encode\", type=\"bool\", default=False,\n",
    "                      help=\"\"\"\\\n",
    "                      Whether to split each word or bpe into character, and then\n",
    "                      generate the word-level representation from the character\n",
    "                      reprentation.\n",
    "                      \"\"\")\n",
    "\n",
    "  # Misc\n",
    "  parser.add_argument(\"--num_gpus\", type=int, default=1,\n",
    "                      help=\"Number of gpus in each worker.\")\n",
    "  parser.add_argument(\"--log_device_placement\", type=\"bool\", nargs=\"?\",\n",
    "                      const=True, default=False, help=\"Debug GPU allocation.\")\n",
    "  parser.add_argument(\"--metrics\", type=str, default=\"bleu\",\n",
    "                      help=(\"Comma-separated list of evaluations \"\n",
    "                            \"metrics (bleu,rouge,accuracy)\"))\n",
    "  parser.add_argument(\"--steps_per_external_eval\", type=int, default=None,\n",
    "                      help=\"\"\"\\\n",
    "      How many training steps to do per external evaluation.  Automatically set\n",
    "      based on data if None.\\\n",
    "      \"\"\")\n",
    "  parser.add_argument(\"--scope\", type=str, default=None,\n",
    "                      help=\"scope to put variables under\")\n",
    "  parser.add_argument(\"--hparams_path\", type=str, default=None,\n",
    "                      help=(\"Path to standard hparams json file that overrides\"\n",
    "                            \"hparams values from FLAGS.\"))\n",
    "  parser.add_argument(\"--random_seed\", type=int, default=None,\n",
    "                      help=\"Random seed (>0, set a specific seed).\")\n",
    "  parser.add_argument(\"--override_loaded_hparams\", type=\"bool\", nargs=\"?\",\n",
    "                      const=True, default=False,\n",
    "                      help=\"Override loaded hparams with values specified\")\n",
    "  parser.add_argument(\"--num_keep_ckpts\", type=int, default=5,\n",
    "                      help=\"Max number of checkpoints to keep.\")\n",
    "  parser.add_argument(\"--avg_ckpts\", type=\"bool\", nargs=\"?\",\n",
    "                      const=True, default=False, help=(\"\"\"\\\n",
    "                      Average the last N checkpoints for external evaluation.\n",
    "                      N can be controlled by setting --num_keep_ckpts.\\\n",
    "                      \"\"\"))\n",
    "  parser.add_argument(\"--language_model\", type=\"bool\", nargs=\"?\",\n",
    "                      const=True, default=False,\n",
    "                      help=\"True to train a language model, ignoring encoder\")\n",
    "\n",
    "  # Inference\n",
    "  parser.add_argument(\"--ckpt\", type=str, default=\"\",\n",
    "                      help=\"Checkpoint file to load a model for inference.\")\n",
    "  parser.add_argument(\"--inference_input_file\", type=str, default=None,\n",
    "                      help=\"Set to the text to decode.\")\n",
    "  parser.add_argument(\"--inference_list\", type=str, default=None,\n",
    "                      help=(\"A comma-separated list of sentence indices \"\n",
    "                            \"(0-based) to decode.\"))\n",
    "  parser.add_argument(\"--infer_batch_size\", type=int, default=32,\n",
    "                      help=\"Batch size for inference mode.\")\n",
    "  parser.add_argument(\"--inference_output_file\", type=str, default=None,\n",
    "                      help=\"Output file to store decoding results.\")\n",
    "  parser.add_argument(\"--inference_ref_file\", type=str, default=None,\n",
    "                      help=(\"\"\"\\\n",
    "      Reference file to compute evaluation scores (if provided).\\\n",
    "      \"\"\"))\n",
    "\n",
    "  # Advanced inference arguments\n",
    "  parser.add_argument(\"--infer_mode\", type=str, default=\"greedy\",\n",
    "                      choices=[\"greedy\", \"sample\", \"beam_search\"],\n",
    "                      help=\"Which type of decoder to use during inference.\")\n",
    "  parser.add_argument(\"--beam_width\", type=int, default=0,\n",
    "                      help=(\"\"\"\\\n",
    "      beam width when using beam search decoder. If 0 (default), use standard\n",
    "      decoder with greedy helper.\\\n",
    "      \"\"\"))\n",
    "  parser.add_argument(\"--length_penalty_weight\", type=float, default=0.0,\n",
    "                      help=\"Length penalty for beam search.\")\n",
    "  parser.add_argument(\"--sampling_temperature\", type=float,\n",
    "                      default=0.0,\n",
    "                      help=(\"\"\"\\\n",
    "      Softmax sampling temperature for inference decoding, 0.0 means greedy\n",
    "      decoding. This option is ignored when using beam search.\\\n",
    "      \"\"\"))\n",
    "  parser.add_argument(\"--num_translations_per_input\", type=int, default=1,\n",
    "                      help=(\"\"\"\\\n",
    "      Number of translations generated for each sentence. This is only used for\n",
    "      inference.\\\n",
    "      \"\"\"))\n",
    "\n",
    "  # Job info\n",
    "  parser.add_argument(\"--jobid\", type=int, default=0,\n",
    "                      help=\"Task id of the worker.\")\n",
    "  parser.add_argument(\"--num_workers\", type=int, default=1,\n",
    "                      help=\"Number of workers (inference only).\")\n",
    "  parser.add_argument(\"--num_inter_threads\", type=int, default=0,\n",
    "                      help=\"number of inter_op_parallelism_threads\")\n",
    "  parser.add_argument(\"--num_intra_threads\", type=int, default=0,\n",
    "                      help=\"number of intra_op_parallelism_threads\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_compatible_hparams(hparams, default_hparams, hparams_path=\"\"):\n",
    "  \"\"\"Make sure the loaded hparams is compatible with new changes.\"\"\"\n",
    "  default_hparams = utils.maybe_parse_standard_hparams(\n",
    "      default_hparams, hparams_path)\n",
    "\n",
    "  # Set num encoder/decoder layers (for old checkpoints)\n",
    "  if hasattr(hparams, \"num_layers\"):\n",
    "    if not hasattr(hparams, \"num_encoder_layers\"):\n",
    "      hparams.add_hparam(\"num_encoder_layers\", hparams.num_layers)\n",
    "    if not hasattr(hparams, \"num_decoder_layers\"):\n",
    "      hparams.add_hparam(\"num_decoder_layers\", hparams.num_layers)\n",
    "\n",
    "  # For compatible reason, if there are new fields in default_hparams,\n",
    "  #   we add them to the current hparams\n",
    "  default_config = default_hparams.values()\n",
    "  config = hparams.values()\n",
    "  for key in default_config:\n",
    "    if key not in config:\n",
    "      hparams.add_hparam(key, default_config[key])\n",
    "\n",
    "  # Update all hparams' keys if override_loaded_hparams=True\n",
    "  if getattr(default_hparams, \"override_loaded_hparams\", None):\n",
    "    overwritten_keys = default_config.keys()\n",
    "  else:\n",
    "    # For inference\n",
    "    overwritten_keys = INFERENCE_KEYS\n",
    "\n",
    "  for key in overwritten_keys:\n",
    "    if getattr(hparams, key) != default_config[key]:\n",
    "      utils.print_out(\"# Updating hparams.%s: %s -> %s\" %\n",
    "                      (key, str(getattr(hparams, key)),\n",
    "                       str(default_config[key])))\n",
    "      setattr(hparams, key, default_config[key])\n",
    "  return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_hparams(hparams):\n",
    "  \"\"\"Add new arguments to hparams.\"\"\"\n",
    "  # Sanity checks\n",
    "  if hparams.encoder_type == \"bi\" and hparams.num_encoder_layers % 2 != 0:\n",
    "    raise ValueError(\"For bi, num_encoder_layers %d should be even\" %\n",
    "                     hparams.num_encoder_layers)\n",
    "  if (hparams.attention_architecture in [\"gnmt\"] and\n",
    "      hparams.num_encoder_layers < 2):\n",
    "    raise ValueError(\"For gnmt attention architecture, \"\n",
    "                     \"num_encoder_layers %d should be >= 2\" %\n",
    "                     hparams.num_encoder_layers)\n",
    "  if hparams.subword_option and hparams.subword_option not in [\"spm\", \"bpe\"]:\n",
    "    raise ValueError(\"subword option must be either spm, or bpe\")\n",
    "  if hparams.infer_mode == \"beam_search\" and hparams.beam_width <= 0:\n",
    "    raise ValueError(\"beam_width must greater than 0 when using beam_search\"\n",
    "                     \"decoder.\")\n",
    "  if hparams.infer_mode == \"sample\" and hparams.sampling_temperature <= 0.0:\n",
    "    raise ValueError(\"sampling_temperature must greater than 0.0 when using\"\n",
    "                     \"sample decoder.\")\n",
    "\n",
    "  # Different number of encoder / decoder layers\n",
    "  assert hparams.num_encoder_layers and hparams.num_decoder_layers\n",
    "  if hparams.num_encoder_layers != hparams.num_decoder_layers:\n",
    "    hparams.pass_hidden_state = False\n",
    "    utils.print_out(\"Num encoder layer %d is different from num decoder layer\"\n",
    "                    \" %d, so set pass_hidden_state to False\" % (\n",
    "                        hparams.num_encoder_layers,\n",
    "                        hparams.num_decoder_layers))\n",
    "\n",
    "  # Set residual layers\n",
    "  num_encoder_residual_layers = 0\n",
    "  num_decoder_residual_layers = 0\n",
    "  if hparams.residual:\n",
    "    if hparams.num_encoder_layers > 1:\n",
    "      num_encoder_residual_layers = hparams.num_encoder_layers - 1\n",
    "    if hparams.num_decoder_layers > 1:\n",
    "      num_decoder_residual_layers = hparams.num_decoder_layers - 1\n",
    "\n",
    "    if hparams.encoder_type == \"gnmt\":\n",
    "      # The first unidirectional layer (after the bi-directional layer) in\n",
    "      # the GNMT encoder can't have residual connection due to the input is\n",
    "      # the concatenation of fw_cell and bw_cell's outputs.\n",
    "      num_encoder_residual_layers = hparams.num_encoder_layers - 2\n",
    "\n",
    "      # Compatible for GNMT models\n",
    "      if hparams.num_encoder_layers == hparams.num_decoder_layers:\n",
    "        num_decoder_residual_layers = num_encoder_residual_layers\n",
    "  _add_argument(hparams, \"num_encoder_residual_layers\",\n",
    "                num_encoder_residual_layers)\n",
    "  _add_argument(hparams, \"num_decoder_residual_layers\",\n",
    "                num_decoder_residual_layers)\n",
    "\n",
    "  # Language modeling\n",
    "  if getattr(hparams, \"language_model\", None):\n",
    "    hparams.attention = \"\"\n",
    "    hparams.attention_architecture = \"\"\n",
    "    hparams.pass_hidden_state = False\n",
    "    hparams.share_vocab = True\n",
    "    hparams.src = hparams.tgt\n",
    "    utils.print_out(\"For language modeling, we turn off attention and \"\n",
    "                    \"pass_hidden_state; turn on share_vocab; set src to tgt.\")\n",
    "\n",
    "  ## Vocab\n",
    "  # Get vocab file names first\n",
    "  if hparams.vocab_prefix:\n",
    "    src_vocab_file = hparams.vocab_prefix + \".\" + hparams.src\n",
    "    tgt_vocab_file = hparams.vocab_prefix + \".\" + hparams.tgt\n",
    "  else:\n",
    "    raise ValueError(\"hparams.vocab_prefix must be provided.\")\n",
    "\n",
    "  # Source vocab\n",
    "  check_special_token = getattr(hparams, \"check_special_token\", True)\n",
    "  src_vocab_size, src_vocab_file = vocab_utils.check_vocab(\n",
    "      src_vocab_file,\n",
    "      hparams.out_dir,\n",
    "      check_special_token=check_special_token,\n",
    "      sos=hparams.sos,\n",
    "      eos=hparams.eos,\n",
    "      unk=vocab_utils.UNK)\n",
    "\n",
    "  # Target vocab\n",
    "  if hparams.share_vocab:\n",
    "    utils.print_out(\"  using source vocab for target\")\n",
    "    tgt_vocab_file = src_vocab_file\n",
    "    tgt_vocab_size = src_vocab_size\n",
    "  else:\n",
    "    tgt_vocab_size, tgt_vocab_file = vocab_utils.check_vocab(\n",
    "        tgt_vocab_file,\n",
    "        hparams.out_dir,\n",
    "        check_special_token=check_special_token,\n",
    "        sos=hparams.sos,\n",
    "        eos=hparams.eos,\n",
    "        unk=vocab_utils.UNK)\n",
    "  _add_argument(hparams, \"src_vocab_size\", src_vocab_size)\n",
    "  _add_argument(hparams, \"tgt_vocab_size\", tgt_vocab_size)\n",
    "  _add_argument(hparams, \"src_vocab_file\", src_vocab_file)\n",
    "  _add_argument(hparams, \"tgt_vocab_file\", tgt_vocab_file)\n",
    "\n",
    "  # Num embedding partitions\n",
    "  num_embeddings_partitions = getattr(hparams, \"num_embeddings_partitions\", 0)\n",
    "  _add_argument(hparams, \"num_enc_emb_partitions\", num_embeddings_partitions)\n",
    "  _add_argument(hparams, \"num_dec_emb_partitions\", num_embeddings_partitions)\n",
    "\n",
    "  # Pretrained Embeddings\n",
    "  _add_argument(hparams, \"src_embed_file\", \"\")\n",
    "  _add_argument(hparams, \"tgt_embed_file\", \"\")\n",
    "  if getattr(hparams, \"embed_prefix\", None):\n",
    "    src_embed_file = hparams.embed_prefix + \".\" + hparams.src\n",
    "    tgt_embed_file = hparams.embed_prefix + \".\" + hparams.tgt\n",
    "\n",
    "    if tf.gfile.Exists(src_embed_file):\n",
    "      utils.print_out(\"  src_embed_file %s exist\" % src_embed_file)\n",
    "      hparams.src_embed_file = src_embed_file\n",
    "\n",
    "      utils.print_out(\n",
    "          \"For pretrained embeddings, set num_enc_emb_partitions to 1\")\n",
    "      hparams.num_enc_emb_partitions = 1\n",
    "    else:\n",
    "      utils.print_out(\"  src_embed_file %s doesn't exist\" % src_embed_file)\n",
    "\n",
    "    if tf.gfile.Exists(tgt_embed_file):\n",
    "      utils.print_out(\"  tgt_embed_file %s exist\" % tgt_embed_file)\n",
    "      hparams.tgt_embed_file = tgt_embed_file\n",
    "\n",
    "      utils.print_out(\n",
    "          \"For pretrained embeddings, set num_dec_emb_partitions to 1\")\n",
    "      hparams.num_dec_emb_partitions = 1\n",
    "    else:\n",
    "      utils.print_out(\"  tgt_embed_file %s doesn't exist\" % tgt_embed_file)\n",
    "\n",
    "  # Evaluation\n",
    "  for metric in hparams.metrics:\n",
    "    best_metric_dir = os.path.join(hparams.out_dir, \"best_\" + metric)\n",
    "    tf.gfile.MakeDirs(best_metric_dir)\n",
    "    _add_argument(hparams, \"best_\" + metric, 0, update=False)\n",
    "    _add_argument(hparams, \"best_\" + metric + \"_dir\", best_metric_dir)\n",
    "\n",
    "    if getattr(hparams, \"avg_ckpts\", None):\n",
    "      best_metric_dir = os.path.join(hparams.out_dir, \"avg_best_\" + metric)\n",
    "      tf.gfile.MakeDirs(best_metric_dir)\n",
    "      _add_argument(hparams, \"avg_best_\" + metric, 0, update=False)\n",
    "      _add_argument(hparams, \"avg_best_\" + metric + \"_dir\", best_metric_dir)\n",
    "\n",
    "  return hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_argument(hparams, key, value, update=True):\n",
    "  \"\"\"Add an argument to hparams; if exists, change the value if update==True.\"\"\"\n",
    "  if hasattr(hparams, key):\n",
    "    if update:\n",
    "      setattr(hparams, key, value)\n",
    "  else:\n",
    "    hparams.add_hparam(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_KEYS = [\"src_max_len_infer\", \"tgt_max_len_infer\", \"subword_option\",\n",
    "                  \"infer_batch_size\", \"beam_width\",\n",
    "                  \"length_penalty_weight\", \"sampling_temperature\",\n",
    "                  \"num_translations_per_input\", \"infer_mode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_parser = argparse.ArgumentParser()\n",
    "add_arguments(nmt_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS, unparsed = nmt_parser.parse_known_args()\n",
    "default_hparams = create_hparams(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_path = 'nmt/standard_hparams/wmt16_gnmt_4_layer.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Loading hparams from /home/emrys/Data/nmt/deen_gnmt/hparams\n",
      "# Loading standard hparams from nmt/standard_hparams/wmt16_gnmt_4_layer.json\n",
      "# Vocab file /home/emrys/Data/nmt/wmt16/vocab.bpe.32000.de exists\n",
      "# Vocab file /home/emrys/Data/nmt/wmt16/vocab.bpe.32000.en exists\n",
      "  attention=normed_bahdanau\n",
      "  attention_architecture=gnmt_v2\n",
      "  avg_ckpts=False\n",
      "  batch_size=128\n",
      "  beam_width=10\n",
      "  best_bleu=26.04670127210219\n",
      "  best_bleu_dir=/home/emrys/Data/nmt/deen_gnmt/best_bleu\n",
      "  check_special_token=True\n",
      "  colocate_gradients_with_ops=True\n",
      "  decay_scheme=luong10\n",
      "  dev_prefix=/home/emrys/Data/nmt/wmt16/newstest2013.tok.bpe.32000\n",
      "  dropout=0.2\n",
      "  embed_prefix=None\n",
      "  encoder_type=gnmt\n",
      "  eos=</s>\n",
      "  epoch_step=19332\n",
      "  forget_bias=1.0\n",
      "  infer_batch_size=32\n",
      "  infer_mode=greedy\n",
      "  init_op=uniform\n",
      "  init_weight=0.1\n",
      "  language_model=False\n",
      "  learning_rate=1.0\n",
      "  length_penalty_weight=1.0\n",
      "  log_device_placement=False\n",
      "  max_gradient_norm=5.0\n",
      "  max_train=0\n",
      "  metrics=['bleu']\n",
      "  num_buckets=5\n",
      "  num_dec_emb_partitions=0\n",
      "  num_decoder_layers=4\n",
      "  num_decoder_residual_layers=2\n",
      "  num_embeddings_partitions=0\n",
      "  num_enc_emb_partitions=0\n",
      "  num_encoder_layers=4\n",
      "  num_encoder_residual_layers=2\n",
      "  num_gpus=1\n",
      "  num_inter_threads=0\n",
      "  num_intra_threads=0\n",
      "  num_keep_ckpts=5\n",
      "  num_sampled_softmax=0\n",
      "  num_train_steps=340000\n",
      "  num_translations_per_input=1\n",
      "  num_units=1024\n",
      "  optimizer=sgd\n",
      "  out_dir=/home/emrys/Data/nmt/deen_gnmt\n",
      "  output_attention=True\n",
      "  override_loaded_hparams=False\n",
      "  pass_hidden_state=True\n",
      "  random_seed=None\n",
      "  residual=True\n",
      "  sampling_temperature=0.0\n",
      "  share_vocab=False\n",
      "  sos=<s>\n",
      "  src=de\n",
      "  src_embed_file=\n",
      "  src_max_len=50\n",
      "  src_max_len_infer=None\n",
      "  src_vocab_file=/home/emrys/Data/nmt/wmt16/vocab.bpe.32000.de\n",
      "  src_vocab_size=36548\n",
      "  steps_per_external_eval=None\n",
      "  steps_per_stats=100\n",
      "  subword_option=bpe\n",
      "  test_prefix=None\n",
      "  tgt=en\n",
      "  tgt_embed_file=\n",
      "  tgt_max_len=50\n",
      "  tgt_max_len_infer=None\n",
      "  tgt_vocab_file=/home/emrys/Data/nmt/wmt16/vocab.bpe.32000.en\n",
      "  tgt_vocab_size=36548\n",
      "  time_major=True\n",
      "  train_prefix=/home/emrys/Data/nmt/wmt16/train.tok.clean.bpe.32000\n",
      "  unit_type=lstm\n",
      "  use_char_encode=False\n",
      "  vocab_prefix=/home/emrys/Data/nmt/wmt16/vocab.bpe.32000\n",
      "  warmup_scheme=t2t\n",
      "  warmup_steps=0\n"
     ]
    }
   ],
   "source": [
    "## see this function as this one have multiple errors\n",
    "hparams = create_or_load_hparams(ckpt_dir, default_hparams=default_hparams, hparams_path=hparams_path,\n",
    "                                 save_hparams=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Creating infer graph ...\n",
      "# Build a GNMT encoder\n",
      "  num_bi_layers = 1\n",
      "  num_uni_layers = 3\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  ResidualWrapper  DeviceWrapper, device=/gpu:0\n",
      "  cell 2  LSTM, forget_bias=1  ResidualWrapper  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 2  LSTM, forget_bias=1  ResidualWrapper  DeviceWrapper, device=/gpu:0\n",
      "  cell 3  LSTM, forget_bias=1  ResidualWrapper  DeviceWrapper, device=/gpu:0\n",
      "  decoder: infer_mode=greedybeam_width=10, length_penalty=1.000000\n",
      "# Trainable variables\n",
      "Format: <name>, <shape>, <(soft) device placement>\n",
      "  embeddings/encoder/embedding_encoder:0, (36548, 1024), /device:GPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (36548, 1024), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/kernel:0, (2048, 4096), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/bias:0, (4096,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/kernel:0, (2048, 4096), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/bias:0, (4096,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (3072, 4096), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (4096,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (2048, 4096), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (4096,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (2048, 4096), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (4096,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (1024, 1024), \n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/basic_lstm_cell/kernel:0, (3072, 4096), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/basic_lstm_cell/bias:0, (4096,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/bahdanau_attention/query_layer/kernel:0, (1024, 1024), \n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/bahdanau_attention/attention_v:0, (1024,), \n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/bahdanau_attention/attention_g:0, (), \n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/bahdanau_attention/attention_b:0, (1024,), \n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (3072, 4096), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (4096,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0, (3072, 4096), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0, (4096,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0, (3072, 4096), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0, (4096,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (1024, 36548), \n",
      "INFO:tensorflow:Restoring parameters from /home/emrys/Data/nmt/deen_gnmt/translate.ckpt-163000\n",
      "  loaded infer model parameters from /home/emrys/Data/nmt/deen_gnmt/translate.ckpt-163000, time 0.55s\n",
      "# Start decoding\n",
      "  decoding to output /tmp/output_infer\n",
      "  done, num sentences 3003, num translations per input 1, time 41s, Tue Nov 13 21:27:58 2018.\n"
     ]
    }
   ],
   "source": [
    "inference('/home/emrys/Data/nmt/deen_gnmt/translate.ckpt-163000', \n",
    "          '/home/emrys/Data/nmt/wmt16/newstest2014.tok.bpe.32000.de',\n",
    "          '/tmp/output_infer', \n",
    "          hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.inference_indices = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
